{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/fluid/source/css/gitalk.css","path":"css/gitalk.css","modified":0,"renderable":1},{"_id":"themes/fluid/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/avatar.png","path":"img/avatar.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/favicon.png","path":"img/favicon.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/default.png","path":"img/default.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/homepage.jpg","path":"img/homepage.jpg","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/homepage.png","path":"img/homepage.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/loading.gif","path":"img/loading.gif","modified":0,"renderable":1},{"_id":"themes/fluid/source/img/police_beian.png","path":"img/police_beian.png","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/boot.js","path":"js/boot.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/color-schema.js","path":"js/color-schema.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/events.js","path":"js/events.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/lazyload.js","path":"js/lazyload.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/leancloud.js","path":"js/leancloud.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/plugins.js","path":"js/plugins.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/js/debouncer.js","path":"js/debouncer.js","modified":0,"renderable":1},{"_id":"themes/fluid/source/xml/local-search.xml","path":"xml/local-search.xml","modified":0,"renderable":1},{"_id":"themes/fluid/source/lib/hint/hint.min.css","path":"lib/hint/hint.min.css","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"acad91ace80b80295b11a9b7ad4c29a2dcfdd8fb","modified":1612507810894},{"_id":"source/about/index.md","hash":"987ca606b88479b9839701f6be26a7a5f7d56b75","modified":1612582454483},{"_id":"source/_posts/1. 线性回归/1.1. 什么是机器学习.md","hash":"4f5cb71a58dc9721619b8b23b1e7fcdc6d753921","modified":1612585246786},{"_id":"source/_posts/1. 线性回归/1.3. 多变量预测.md","hash":"cb64b16d9574697be9fffeaa54118f8b50472145","modified":1612585201088},{"_id":"source/_posts/1. 线性回归/1.2. 梯度下降算法.md","hash":"f80c8e89fbdeecf498d7e340a2976ce9aa3fde0c","modified":1612585897609},{"_id":"source/_posts/1. 线性回归/1.4. 调试方法.md","hash":"4bfb83ee329461b3b03219e82f7f27c83ad07e1b","modified":1612585144268},{"_id":"source/_posts/1. 线性回归/1.5. 多项式拟合和正规方程.md","hash":"f300ca8e6496edf90faa674bd0cdec37cecbc5af","modified":1612585152702},{"_id":"themes/fluid/source/css/_pages/_category/category.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1612582163919},{"_id":"themes/fluid/source/css/_pages/_tag/tag.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1612582163931},{"_id":"themes/fluid/.editorconfig","hash":"b595159772f3ee1ef5e6780ce307270e741cb309","modified":1612582163740},{"_id":"themes/fluid/.eslintrc","hash":"3df89453e1f63051fafc90f16a8d83951050e316","modified":1612582163744},{"_id":"themes/fluid/.gitattributes","hash":"3e00e1fb043438cd820d94ee3dc9ffb6718996f3","modified":1612582163745},{"_id":"themes/fluid/.gitignore","hash":"a7ebb30cbe321e793e0333d6b83c8595fd63e151","modified":1612585886169},{"_id":"themes/fluid/LICENSE","hash":"653274b0f005f82768a7e5e17a32d38bfe38d492","modified":1612582163768},{"_id":"themes/fluid/README_en.md","hash":"2b47ed68ec888dcc34fa6aad9ce95aeba6744fec","modified":1612582163771},{"_id":"themes/fluid/README.md","hash":"f75577ec5abe2033077dae29354d46041c7aeb96","modified":1612582163769},{"_id":"themes/fluid/_config.yml","hash":"d6a12684bde3e6b37c3e58280ecdd71d6c85dd6c","modified":1612583939983},{"_id":"themes/fluid/gulpfile.js","hash":"1ad8861ce4f702c164f908efb6c1c6504154ed2d","modified":1612582163775},{"_id":"themes/fluid/package.json","hash":"c8f6fb1d1a06785f0dbe3b24d02d07075d38e7a7","modified":1612582163843},{"_id":"themes/fluid/languages/de.yml","hash":"288f649c2c2314eb610693b18853ee74f0541e87","modified":1612582163779},{"_id":"themes/fluid/languages/en.yml","hash":"31f2867619a768606166778d4ee51f3d00ac33a0","modified":1612582163781},{"_id":"themes/fluid/languages/ja.yml","hash":"dc43be11a300893ebef47283c22f2f946ca21260","modified":1612582163782},{"_id":"themes/fluid/languages/zh-CN.yml","hash":"3b92f4428d66c31610f6cde13b82ee723aff00d2","modified":1612582163782},{"_id":"themes/fluid/layout/404.ejs","hash":"79a598e43c40d48b23076361720f1e77df466e41","modified":1612582163785},{"_id":"themes/fluid/layout/archive.ejs","hash":"7a1e19dec37804927f0d331d7e6c80ed03becd61","modified":1612582163828},{"_id":"themes/fluid/layout/about.ejs","hash":"22acb19c193fd215d0f5d5668fbf5ca5f5407388","modified":1612582163827},{"_id":"themes/fluid/layout/categories.ejs","hash":"0a8fe294cbbcc5112e360fcfdef5925fd39580bb","modified":1612582163830},{"_id":"themes/fluid/layout/index.ejs","hash":"89aecee3059b4631d22e0fb1b8594b5217d933cf","modified":1612582163833},{"_id":"themes/fluid/layout/category.ejs","hash":"dd2bd15cbd811d6ea973b6e6a17d99e36151e274","modified":1612582163830},{"_id":"themes/fluid/layout/layout.ejs","hash":"94352823b3bb8cf2bb91489f7e965434e04dd8ad","modified":1612582163835},{"_id":"themes/fluid/layout/links.ejs","hash":"8efaa2ab9804df1b3f72b6940b47247eb7853b66","modified":1612582163837},{"_id":"themes/fluid/layout/page.ejs","hash":"9f4a66735082127fd0792ba747029212e63c90a1","modified":1612582163838},{"_id":"themes/fluid/layout/post.ejs","hash":"22b0eac55d43028bd8ec17f0b6855868adce55aa","modified":1612582163840},{"_id":"themes/fluid/layout/tag.ejs","hash":"3a9296eb7181e8b3fb0cdc60cbafc815b98d6f51","modified":1612582163841},{"_id":"themes/fluid/layout/tags.ejs","hash":"b7c1a6d8fc1097fc16d2300260297013cb692153","modified":1612582163842},{"_id":"themes/fluid/.github/workflows/limit.yaml","hash":"bdbdb66da69ab7353b546f02150a6792f4787975","modified":1612582163763},{"_id":"themes/fluid/.github/workflows/lint.yaml","hash":"17f7d9aba4420011b7665275c58f838557d2434c","modified":1612582163764},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/bug_report.md","hash":"beb3474d6f65c1e56700ba872c6a0d0836d4168e","modified":1612582163748},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/bug_report_zh.md","hash":"78ce211415d502c5a4398d786d5c697d34d868b9","modified":1612582163751},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/feature_request.md","hash":"5cc30e7b6e7b77c8b40b182ba02a5d93d37d2fc2","modified":1612582163753},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/feature_request_zh.md","hash":"7db378613df2b7d13e8c428c006399a879a4a852","modified":1612582163755},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/question.md","hash":"102213e5d6790d060c0e26b4a3a7ec744d753c52","modified":1612582163756},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/question_zh.md","hash":"18381d03518526d7cefd024a0bdd8d9e7c6440f5","modified":1612582163759},{"_id":"themes/fluid/layout/_partial/archive-list.ejs","hash":"7d780309e12c437c2f8a246dd2fd0c272b8636ce","modified":1612582163787},{"_id":"themes/fluid/layout/_partial/beian.ejs","hash":"53d9f79b4a3b71d2e89872fa138bc09611862ee4","modified":1612582163789},{"_id":"themes/fluid/layout/_partial/css.ejs","hash":"23ead7d3e6c9a86ca0076d0ef7db98f3bb4c9b08","modified":1612582163804},{"_id":"themes/fluid/layout/_partial/footer.ejs","hash":"a886be025d5e3b2c49657ba30e459f980a7cc75e","modified":1612582163805},{"_id":"themes/fluid/layout/_partial/head.ejs","hash":"e0a374abd17d436e47c224519abe889050859158","modified":1612582163806},{"_id":"themes/fluid/layout/_partial/nav.ejs","hash":"a2c16035ea5690882159f9df3a9ddb77ea74b4af","modified":1612582163809},{"_id":"themes/fluid/layout/_partial/paginator.ejs","hash":"8191c630b3db4dd1dd8aad56defad93334a7b691","modified":1612582163810},{"_id":"themes/fluid/layout/_partial/post-meta.ejs","hash":"3f16de8c40d87c7d23eba121dd8061757c3f9a58","modified":1612582163820},{"_id":"themes/fluid/layout/_partial/scripts.ejs","hash":"7026b27fe961f8f821757b84340a55ffdce7e37d","modified":1612582163822},{"_id":"themes/fluid/layout/_partial/search.ejs","hash":"bea21f1b5de61badd6c068080315c201fc80bc36","modified":1612582163823},{"_id":"themes/fluid/layout/_partial/statistics.ejs","hash":"a70c26e415a27f07f38b9384e7eb48d1f2b30328","modified":1612582163823},{"_id":"themes/fluid/layout/_partial/toc.ejs","hash":"76e6bc368cf46d4103ea9514699e10ec0b9a4b56","modified":1612582163825},{"_id":"themes/fluid/scripts/events/index.js","hash":"68cf640a98017a4d569289e99ae94bb4a3595f92","modified":1612582163846},{"_id":"themes/fluid/scripts/helpers/export-config.js","hash":"42b969bb7fe5356d22f42e54b107724ae1e72e4d","modified":1612582163867},{"_id":"themes/fluid/scripts/helpers/page.js","hash":"49b2c6449d7be35739c6cfea3cab4e790580983a","modified":1612582163868},{"_id":"themes/fluid/scripts/helpers/url.js","hash":"1664f8faa028898bd6f91d6db61c7dbf7463ee01","modified":1612582163869},{"_id":"themes/fluid/scripts/helpers/utils.js","hash":"4acb213f90f1e7ba3696ef08d894a2a84807b669","modified":1612582163871},{"_id":"themes/fluid/scripts/helpers/wordcount.js","hash":"da6144ee040fed0a9b9f45da3478bc33087b5ac9","modified":1612582163872},{"_id":"themes/fluid/scripts/filters/post-filter.js","hash":"1827cb42259dbfff1d072c5c8b388bf7d76e6acd","modified":1612582163860},{"_id":"themes/fluid/scripts/filters/locals.js","hash":"2340a576635b16fd2456b3494f5afe89cd7764db","modified":1612582163858},{"_id":"themes/fluid/scripts/generators/local-search.js","hash":"bda7fbe58082a2a02c0db066794b791b14462271","modified":1612582163863},{"_id":"themes/fluid/scripts/generators/pages.js","hash":"a2a15ea722863aba09dcad578558432682a3b6b3","modified":1612582163864},{"_id":"themes/fluid/scripts/tags/button.js","hash":"e1d0caed12e7cd9a35cf64272c41854b2901a58f","modified":1612582163874},{"_id":"themes/fluid/scripts/tags/checkbox.js","hash":"dac0e08eaa3614a6fd9ddbdfb4584094b1bdb30a","modified":1612582163875},{"_id":"themes/fluid/scripts/tags/group-image.js","hash":"cc176cc1d7e7cc28cedf8397ae748c691d140be2","modified":1612582163877},{"_id":"themes/fluid/scripts/tags/label.js","hash":"6c5916d86c63795c7e910bf614b0e7ece5073702","modified":1612582163877},{"_id":"themes/fluid/scripts/tags/mermaid.js","hash":"dbfe59fde77d87b1d7d0c46480a2a729010988eb","modified":1612582163880},{"_id":"themes/fluid/scripts/tags/note.js","hash":"8020acc2c4bb3a2054e3cb349fac7cd10b79a0be","modified":1612582163881},{"_id":"themes/fluid/scripts/utils/join-path.js","hash":"ec068c699155565aea4aa4ab55d8a10b2947a114","modified":1612582163883},{"_id":"themes/fluid/scripts/utils/object.js","hash":"d07abe58481ab097fc4e5b9f573cdc46bce4bfec","modified":1612582163884},{"_id":"themes/fluid/source/css/gitalk.css","hash":"1fe60b2ab1d704f5a4f55e700dca5b8785fb390e","modified":1612582163938},{"_id":"themes/fluid/source/css/main.styl","hash":"bf536db598434c36cc0c752196bfde46e584a92e","modified":1612582163939},{"_id":"themes/fluid/source/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1612582163941},{"_id":"themes/fluid/source/img/favicon.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1612582163945},{"_id":"themes/fluid/source/img/default.png","hash":"7bb2b8ee07db305bcadee2985b81b942027ae940","modified":1612582163944},{"_id":"themes/fluid/source/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1612582163956},{"_id":"themes/fluid/source/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1612582163958},{"_id":"themes/fluid/source/js/boot.js","hash":"253d4099a093ae0121f5e0ceb1b4ece6d3ce4d33","modified":1612582163961},{"_id":"themes/fluid/source/js/color-schema.js","hash":"eb64f1d74891b0d063d835a9b95b0914d5aced09","modified":1612582163963},{"_id":"themes/fluid/source/js/events.js","hash":"50afea7d926c42e3155f7cf6ab5f2399aca8f746","modified":1612582163965},{"_id":"themes/fluid/source/js/lazyload.js","hash":"91df93084b53aab48500ebb88019c8c523945243","modified":1612582163967},{"_id":"themes/fluid/source/js/leancloud.js","hash":"67c04252b14c1da645952c4e299a00030440a28e","modified":1612582163968},{"_id":"themes/fluid/source/js/local-search.js","hash":"bb899b1ddb2f3c2565d846def2f742ca15407cae","modified":1612582163971},{"_id":"themes/fluid/source/js/plugins.js","hash":"26f935270c5925bcc52b09b0f7ac582312183ea0","modified":1612582163973},{"_id":"themes/fluid/source/js/utils.js","hash":"43787e6704b24acd8f6639a527b7ae18c0adc1b3","modified":1612582163973},{"_id":"themes/fluid/source/js/debouncer.js","hash":"8833902327af7beac17ab97227fcd835329abfa2","modified":1612582163964},{"_id":"themes/fluid/source/xml/local-search.xml","hash":"85fcc23b4db654a7f91fc55b6fb0442bb3ed3a9a","modified":1612582163979},{"_id":"themes/fluid/layout/_partial/comments/changyan.ejs","hash":"fc4890cde550b8e275492ea02b4c5e56e4a08ecf","modified":1612582163791},{"_id":"themes/fluid/layout/_partial/comments/gitalk.ejs","hash":"71564e440e0dc4338d7bb3a6d98790633f0cd640","modified":1612582163794},{"_id":"themes/fluid/layout/_partial/comments/livere.ejs","hash":"9ba4ce6240ff29c4328e58ec88b90392e825289b","modified":1612582163794},{"_id":"themes/fluid/layout/_partial/comments/disqus.ejs","hash":"9c07fa38b30f5d8815ec9fff91c4b6aeed71c301","modified":1612582163793},{"_id":"themes/fluid/layout/_partial/comments/remark42.ejs","hash":"dd757f563edc9e0f2a5efb281bc4985a0a186d58","modified":1612582163796},{"_id":"themes/fluid/layout/_partial/comments/twikoo.ejs","hash":"0bf01d77b3c6eb2831056d6627fdc15b6fcf9ba1","modified":1612582163797},{"_id":"themes/fluid/layout/_partial/comments/utterances.ejs","hash":"8e99361e920a2add4c84a122dd319e72ab5a8c4f","modified":1612582163799},{"_id":"themes/fluid/layout/_partial/comments/valine.ejs","hash":"9a1e040ff2e53c3046cfce915b50729080d74e78","modified":1612582163800},{"_id":"themes/fluid/layout/_partial/comments/waline.ejs","hash":"4a76d773037caf370f22ef8d62e8cfa138628e5a","modified":1612582163803},{"_id":"themes/fluid/layout/_partial/plugins/analytics.ejs","hash":"d20f54bf2fd5cd274b4b9c5542eafbfec5120838","modified":1612582163811},{"_id":"themes/fluid/layout/_partial/plugins/mermaid.ejs","hash":"fd1f78287c868ccab78b6244b66e3f9b0968c4a8","modified":1612582163816},{"_id":"themes/fluid/layout/_partial/plugins/local-search.ejs","hash":"1daab8ac0e67db873816e96cc8535c7640d58e40","modified":1612582163814},{"_id":"themes/fluid/layout/_partial/plugins/math.ejs","hash":"a49a0064b55cf6d8f2a61abfecd41f0083757e04","modified":1612582163815},{"_id":"themes/fluid/layout/_partial/plugins/nprogress.ejs","hash":"47c1df255aa552ad71ef3e57deca46530a8f2802","modified":1612582163818},{"_id":"themes/fluid/layout/_partial/plugins/typed.ejs","hash":"c57817ceaee868d416558e56d2a8d0d418c64a2d","modified":1612582163819},{"_id":"themes/fluid/scripts/events/lib/compatible-configs.js","hash":"7491124c193dcba38ac41aed74f07d10138e6686","modified":1612582163849},{"_id":"themes/fluid/scripts/events/lib/footnote.js","hash":"13d8466cd4c98367131b5f3d6a30b3d4ce8de26f","modified":1612582163850},{"_id":"themes/fluid/scripts/events/lib/hello.js","hash":"1a262c15896663dba773a1796f637f6484f3e524","modified":1612582163851},{"_id":"themes/fluid/scripts/events/lib/lazyload.js","hash":"79cf53f6f53320c6f89494f64649dc792841d0aa","modified":1612582163854},{"_id":"themes/fluid/scripts/events/lib/highlight.js","hash":"5eec946182fd537a4d75f15bdf7a09453cc00d83","modified":1612582163853},{"_id":"themes/fluid/scripts/events/lib/merge-configs.js","hash":"bc6d57fac82bd0ccdac527d07673d563ea1c2af9","modified":1612582163855},{"_id":"themes/fluid/scripts/events/lib/version.js","hash":"8d4d18c027b081a2c4c86b861c1d331aba3cb1a2","modified":1612582163856},{"_id":"themes/fluid/source/css/_functions/base.styl","hash":"171697018fd384fce0834875ca94b91f16564cac","modified":1612582163887},{"_id":"themes/fluid/source/css/_mixins/base.styl","hash":"046979dbd8cdabd21d89f9c1d8f1bb3f2fd06d6f","modified":1612582163889},{"_id":"themes/fluid/source/css/_pages/pages.styl","hash":"92c062cf55457b6549497244d09ec34e9c0c95c2","modified":1612582163933},{"_id":"themes/fluid/source/css/_variables/base.styl","hash":"d56121a37876ff0d21ff0946623b12de91b2d8f5","modified":1612582163936},{"_id":"themes/fluid/source/lib/hint/hint.min.css","hash":"64fa8c328dc93432ec822de2818aef21a4f63b29","modified":1612582163977},{"_id":"themes/fluid/source/css/_pages/_about/about.styl","hash":"47235d222812e2f829e9bde039fa719bbced9325","modified":1612582163892},{"_id":"themes/fluid/source/css/_pages/_archive/archive.styl","hash":"86926a80bf6f39a7f47789b1a8f44b5984b4683f","modified":1612582163893},{"_id":"themes/fluid/source/css/_pages/_category/categories.styl","hash":"0924e35eff2ec84e2d9e4772abccda452d9463ef","modified":1612582163918},{"_id":"themes/fluid/source/css/_pages/_base/base.styl","hash":"574a9c64f071d938a31a768451aad64b2999a1d6","modified":1612582163910},{"_id":"themes/fluid/source/css/_pages/_base/color-schema.styl","hash":"615d35d4d73f1efe114add257eb7c600571029ad","modified":1612582163911},{"_id":"themes/fluid/source/css/_pages/_base/inline.styl","hash":"6f2a3b8af2793dd831f661c6db0ccbe0a62ccc48","modified":1612582163912},{"_id":"themes/fluid/source/css/_pages/_base/keyframes.styl","hash":"58a7f8f2baea2d58cf5f7edfc91314ee5d7156ca","modified":1612582163914},{"_id":"themes/fluid/source/css/_pages/_links/links.styl","hash":"83694b28209c548ef38bee78e473b02e90cbcf9f","modified":1612582163924},{"_id":"themes/fluid/source/css/_pages/_base/rewrite.styl","hash":"f61a303f57f12be0610cc9d37e1f9c73ba442d00","modified":1612582163916},{"_id":"themes/fluid/source/css/_pages/_index/index.styl","hash":"3e8339b19dc168a8154027381ce2616faf4d8e48","modified":1612582163922},{"_id":"themes/fluid/source/css/_pages/_post/post.styl","hash":"cd1308dc0e46a21c47023609dba15bb6d358049e","modified":1612582163928},{"_id":"themes/fluid/source/css/_pages/_post/tag_plugin.styl","hash":"88939a09d1ab73a2b96a6b8b08c96ad03d402728","modified":1612582163929},{"_id":"themes/fluid/source/css/_pages/_tag/tags.styl","hash":"29e9b72cfda2f2baf9cf2597fcd7f9e66303a9bd","modified":1612582163932},{"_id":"themes/fluid/source/css/_pages/_base/_widget/banner.styl","hash":"da823846f0896f16b21c7430f047f7222a89cd10","modified":1612582163896},{"_id":"themes/fluid/source/css/_pages/_base/_widget/board.styl","hash":"bb9cdde191b9b1287ba19414bab862f30be6a8a0","modified":1612582163898},{"_id":"themes/fluid/source/css/_pages/_base/_widget/copy-btn.styl","hash":"c398892fba1494dd6fd417415076458ed321d34d","modified":1612582163900},{"_id":"themes/fluid/source/css/_pages/_base/_widget/footer.styl","hash":"dbd0a3518e5bfca92851490b34654f46bb5cfc76","modified":1612582163901},{"_id":"themes/fluid/source/css/_pages/_base/_widget/footnote.styl","hash":"41935973a66c14ab2bea0539d4b1f15c62534fa4","modified":1612582163903},{"_id":"themes/fluid/source/css/_pages/_base/_widget/header.styl","hash":"f770c5c3ee89421e9e3f1313ca5bd07a2448f400","modified":1612582163904},{"_id":"themes/fluid/source/css/_pages/_base/_widget/qrcode.styl","hash":"d29064ed8bdf62d5cf4eac32ebdb5d0c7075ebbd","modified":1612582163905},{"_id":"themes/fluid/source/css/_pages/_base/_widget/scroll-btn.styl","hash":"5081ec00d3a1ee1a117cf33308abf25d71d133c4","modified":1612582163907},{"_id":"themes/fluid/source/css/_pages/_base/_widget/search.styl","hash":"1f4e678d7219815ab62de1b92ec75e021247f90b","modified":1612582163909},{"_id":"themes/fluid/source/img/homepage.jpg","hash":"b48cbe9c31ea3e5552a1ca9ed448ad3100eeedb6","modified":1612583171717},{"_id":"themes/fluid/source/img/homepage.png","hash":"020a8d14e134b31185581ffa55d608feff76692f","modified":1612583426790},{"_id":"public/local-search.xml","hash":"0d50d7dc9165ea6d071cf4e7fecd16f935c8b8ee","modified":1612586861502},{"_id":"public/about/index.html","hash":"f43efb8227b50cc878d28640986bbdf009dbc659","modified":1612587548900},{"_id":"public/2021/02/06/1. 线性回归/1.4. 调试方法/index.html","hash":"b38033fc73bf2527a2051555d80582b0119cffc9","modified":1612587548900},{"_id":"public/2021/02/06/1. 线性回归/1.3. 多变量预测/index.html","hash":"0eadd3f16beafd539a251f7486b093fdaad96f41","modified":1612587548900},{"_id":"public/index.html","hash":"5185135765e5fb6b3d6be08fa26c70583b86692d","modified":1612587548900},{"_id":"public/2021/02/05/hello-world/index.html","hash":"0e7dcdaf43ec8a7f729822d7411f6cb69bb829e0","modified":1612587548900},{"_id":"public/archives/index.html","hash":"36b93bf7b0d23852b526a675fb9bbdaef785efa7","modified":1612587548900},{"_id":"public/archives/2021/index.html","hash":"36b93bf7b0d23852b526a675fb9bbdaef785efa7","modified":1612587548900},{"_id":"public/404.html","hash":"623a036fd0764660f7d84f0acfcecc1277f56cad","modified":1612587548900},{"_id":"public/archives/2021/02/index.html","hash":"36b93bf7b0d23852b526a675fb9bbdaef785efa7","modified":1612587548900},{"_id":"public/tags/index.html","hash":"4f911c84e87cc3b4e38704036045c1ea97bba8c6","modified":1612587548900},{"_id":"public/categories/index.html","hash":"3e77f8c3e1a5eda32978bce5255a421e31fb0f0c","modified":1612587548900},{"_id":"public/links/index.html","hash":"18bf935b5946ce7d8107465dcf3cdb353b19efa5","modified":1612587548900},{"_id":"public/2021/02/06/1. 线性回归/1.5. 多项式拟合和正规方程/index.html","hash":"445341a56630cd77d30c6143adb1c7551764e043","modified":1612587548900},{"_id":"public/2021/02/06/1. 线性回归/1.2. 梯度下降算法/index.html","hash":"c98df907ac47ddbf12349eb0dd5309bccb6f69ec","modified":1612587548900},{"_id":"public/img/favicon.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1612584997382},{"_id":"public/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1612584997382},{"_id":"public/img/default.png","hash":"7bb2b8ee07db305bcadee2985b81b942027ae940","modified":1612584997382},{"_id":"public/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1612584997382},{"_id":"public/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1612584997382},{"_id":"public/xml/local-search.xml","hash":"85fcc23b4db654a7f91fc55b6fb0442bb3ed3a9a","modified":1612584997382},{"_id":"public/css/gitalk.css","hash":"a57b3cc8e04a0a4a27aefa07facf5b5e7bca0e76","modified":1612584997382},{"_id":"public/js/boot.js","hash":"8b4a0a4fa13121d9c8154a7a2fc7d0a9c8f528f9","modified":1612584997382},{"_id":"public/css/main.css","hash":"713a8e15679fc4b5b835cf938b4f3b62ef27f319","modified":1612584997382},{"_id":"public/js/color-schema.js","hash":"7d7444387e549e06a4a378706df92558de62e4e7","modified":1612584997382},{"_id":"public/js/events.js","hash":"0e998c64d8b03fe6914b3f8fa6990b220ab353bd","modified":1612584997382},{"_id":"public/js/lazyload.js","hash":"0df461660bbd73a79f3125ba4e9bdbc856232e6b","modified":1612584997382},{"_id":"public/js/local-search.js","hash":"13d5ef2fe68c49bd6096781034dbb26c190b5176","modified":1612584997382},{"_id":"public/js/leancloud.js","hash":"4701f49b3dc62939adff5cc11f6d21963df7f135","modified":1612584997382},{"_id":"public/js/plugins.js","hash":"dc1d8485c251a9dc2a1d1f6ffa65c8e7572b8fc9","modified":1612584997382},{"_id":"public/js/utils.js","hash":"3086cede8d6d96ac5f4b5236b06271599a1ebbcb","modified":1612584997382},{"_id":"public/js/debouncer.js","hash":"045f324777bdfb99d4c17b1806169f029f897a65","modified":1612584997382},{"_id":"public/lib/hint/hint.min.css","hash":"b38df228460ebfb4c0b6085336ee2878fe85aafe","modified":1612584997382},{"_id":"public/img/homepage.jpg","hash":"b48cbe9c31ea3e5552a1ca9ed448ad3100eeedb6","modified":1612584997382},{"_id":"public/img/homepage.png","hash":"020a8d14e134b31185581ffa55d608feff76692f","modified":1612584997382},{"_id":"public/2021/02/06/1. 线性回归/1.1. 什么是机器学习/index.html","hash":"aca8ffe434b6de227c40f7bc7a9534dea78cdeab","modified":1612587548900}],"Category":[],"Data":[],"Page":[{"title":"about","date":"2021-02-06T11:33:00.000Z","_content":"\n试着用博客写点东西\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2021-02-06 11:33:00\n\n---\n\n试着用博客写点东西\n","updated":"2021-02-06T03:34:14.483Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckkt7gzv10001880hg4euhmi3","content":"<p>试着用博客写点东西</p>\n","site":{"data":{}},"excerpt":"","more":"<p>试着用博客写点东西</p>\n"}],"Post":[{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2021-02-05T06:50:10.894Z","updated":"2021-02-05T06:50:10.894Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckkt7gzuw0000880h9cv6fbk1","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\">$ hexo new <span class=\"hljs-string\">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\">$ hexo server<br></code></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\">$ hexo generate<br></code></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\">$ hexo deploy<br></code></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\">$ hexo new <span class=\"hljs-string\">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\">$ hexo server<br></code></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\">$ hexo generate<br></code></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\">$ hexo deploy<br></code></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"1.3. 多变量预测","_content":"\n<style>\nimg{\n    width: 30%;\n    padding-left: 20%;\n}\n</style>\n\n# 多变量预测\n\n## 多元线性回归\n\n对于多个特征量(Features)，规定符号表示：\n$n$ 特征的总数量\n$x^{(i)}$  第i个训练样本的输入特征向量，$i$表示的是一个索引(Index)\n$x_j^i$  第i个训练样本中特征向量的第j个值\n\n此时的假设函数不再是单纯的$h_θ (x)=θ_0+θ_1 x$ 的形式。\n对于多个特征量，此时的假设函数为：\n$h_θ (x)=θ^T x=θ_0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)}$\n对这个样本进行简化：\n定义$x_0^i=1$, 定义参数向量：$x=\\begin{bmatrix} x_0\\\\x_1\\\\...\\\\x_n\\end{bmatrix}n$，系数向量：$θ=\\begin{bmatrix}θ_0\\\\θ_1\\\\…\\\\θ_n\\end{bmatrix}$\n有：\n\n$$\nh_θ (x)=θ^T x\n\n$$\n\n这就是假设函数的向量形式。\n\n## 梯度下降算法在多元线性回归中的应用\n\n对于假设函数：$h_θ (x)=θ^T x=θ_0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)}$\n和损失函数：$J(θ_0,θ_1,…,θ_n)=\\frac{1}{2m} ∑_{i=1}^m(h_θ (x^{(i)} )−y^{(i)} )^2$\n此时的梯度下降算法：\nRepeat{\n\n$$\nθ_j≔θ_j−α\\frac{∂J(θ)}{∂θ_j}\n\n$$\n\n}\n对$\\frac{∂J(θ)}{∂θ_j}$进行等价变形：\nRepeat{\n\n$$\nθ_j≔θ_j−α\\frac{1}{m}∑_{i=1}^m(h_θ (x^{(i)} )−y^{(i)})  x_j^i\n\n$$\n\n}\n","source":"_posts/1. 线性回归/1.3. 多变量预测.md","raw":"---\ntitle: 1.3. 多变量预测\n\n---\n\n<style>\nimg{\n    width: 30%;\n    padding-left: 20%;\n}\n</style>\n\n# 多变量预测\n\n## 多元线性回归\n\n对于多个特征量(Features)，规定符号表示：\n$n$ 特征的总数量\n$x^{(i)}$  第i个训练样本的输入特征向量，$i$表示的是一个索引(Index)\n$x_j^i$  第i个训练样本中特征向量的第j个值\n\n此时的假设函数不再是单纯的$h_θ (x)=θ_0+θ_1 x$ 的形式。\n对于多个特征量，此时的假设函数为：\n$h_θ (x)=θ^T x=θ_0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)}$\n对这个样本进行简化：\n定义$x_0^i=1$, 定义参数向量：$x=\\begin{bmatrix} x_0\\\\x_1\\\\...\\\\x_n\\end{bmatrix}n$，系数向量：$θ=\\begin{bmatrix}θ_0\\\\θ_1\\\\…\\\\θ_n\\end{bmatrix}$\n有：\n\n$$\nh_θ (x)=θ^T x\n\n$$\n\n这就是假设函数的向量形式。\n\n## 梯度下降算法在多元线性回归中的应用\n\n对于假设函数：$h_θ (x)=θ^T x=θ_0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)}$\n和损失函数：$J(θ_0,θ_1,…,θ_n)=\\frac{1}{2m} ∑_{i=1}^m(h_θ (x^{(i)} )−y^{(i)} )^2$\n此时的梯度下降算法：\nRepeat{\n\n$$\nθ_j≔θ_j−α\\frac{∂J(θ)}{∂θ_j}\n\n$$\n\n}\n对$\\frac{∂J(θ)}{∂θ_j}$进行等价变形：\nRepeat{\n\n$$\nθ_j≔θ_j−α\\frac{1}{m}∑_{i=1}^m(h_θ (x^{(i)} )−y^{(i)})  x_j^i\n\n$$\n\n}\n","slug":"1. 线性回归/1.3. 多变量预测","published":1,"date":"2021-02-06T03:54:46.300Z","updated":"2021-02-06T04:20:01.088Z","_id":"ckkt7gzv50002880h5cp70ev3","comments":1,"layout":"post","photos":[],"link":"","content":"<style>\nimg{\n    width: 30%;\n    padding-left: 20%;\n}\n</style>\n\n<h1 id=\"多变量预测\"><a href=\"#多变量预测\" class=\"headerlink\" title=\"多变量预测\"></a>多变量预测</h1><h2 id=\"多元线性回归\"><a href=\"#多元线性回归\" class=\"headerlink\" title=\"多元线性回归\"></a>多元线性回归</h2><p>对于多个特征量(Features)，规定符号表示：<br>$n$ 特征的总数量<br>$x^{(i)}$  第i个训练样本的输入特征向量，$i$表示的是一个索引(Index)<br>$x_j^i$  第i个训练样本中特征向量的第j个值</p>\n<p>此时的假设函数不再是单纯的$h<em>θ (x)=θ_0+θ_1 x$ 的形式。<br>对于多个特征量，此时的假设函数为：<br>$h</em>θ (x)=θ^T x=θ_0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)}$<br>对这个样本进行简化：<br>定义$x_0^i=1$, 定义参数向量：$x=\\begin{bmatrix} x_0\\x_1\\…\\x_n\\end{bmatrix}n$，系数向量：$θ=\\begin{bmatrix}θ_0\\θ_1\\…\\θ_n\\end{bmatrix}$<br>有：</p>\n<script type=\"math/tex; mode=display\">\nh_θ (x)=θ^T x</script><p>这就是假设函数的向量形式。</p>\n<h2 id=\"梯度下降算法在多元线性回归中的应用\"><a href=\"#梯度下降算法在多元线性回归中的应用\" class=\"headerlink\" title=\"梯度下降算法在多元线性回归中的应用\"></a>梯度下降算法在多元线性回归中的应用</h2><p>对于假设函数：$h<em>θ (x)=θ^T x=θ_0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)}$<br>和损失函数：$J(θ_0,θ_1,…,θ_n)=\\frac{1}{2m} ∑</em>{i=1}^m(h_θ (x^{(i)} )−y^{(i)} )^2$<br>此时的梯度下降算法：<br>Repeat{</p>\n<script type=\"math/tex; mode=display\">\nθ_j≔θ_j−α\\frac{∂J(θ)}{∂θ_j}</script><p>}<br>对$\\frac{∂J(θ)}{∂θ_j}$进行等价变形：<br>Repeat{</p>\n<script type=\"math/tex; mode=display\">\nθ_j≔θ_j−α\\frac{1}{m}∑_{i=1}^m(h_θ (x^{(i)} )−y^{(i)})  x_j^i</script><p>}</p>\n","site":{"data":{}},"excerpt":"","more":"<style>\nimg{\n    width: 30%;\n    padding-left: 20%;\n}\n</style>\n\n<h1 id=\"多变量预测\"><a href=\"#多变量预测\" class=\"headerlink\" title=\"多变量预测\"></a>多变量预测</h1><h2 id=\"多元线性回归\"><a href=\"#多元线性回归\" class=\"headerlink\" title=\"多元线性回归\"></a>多元线性回归</h2><p>对于多个特征量(Features)，规定符号表示：<br>$n$ 特征的总数量<br>$x^{(i)}$  第i个训练样本的输入特征向量，$i$表示的是一个索引(Index)<br>$x_j^i$  第i个训练样本中特征向量的第j个值</p>\n<p>此时的假设函数不再是单纯的$h<em>θ (x)=θ_0+θ_1 x$ 的形式。<br>对于多个特征量，此时的假设函数为：<br>$h</em>θ (x)=θ^T x=θ_0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)}$<br>对这个样本进行简化：<br>定义$x_0^i=1$, 定义参数向量：$x=\\begin{bmatrix} x_0\\x_1\\…\\x_n\\end{bmatrix}n$，系数向量：$θ=\\begin{bmatrix}θ_0\\θ_1\\…\\θ_n\\end{bmatrix}$<br>有：</p>\n<script type=\"math/tex; mode=display\">\nh_θ (x)=θ^T x</script><p>这就是假设函数的向量形式。</p>\n<h2 id=\"梯度下降算法在多元线性回归中的应用\"><a href=\"#梯度下降算法在多元线性回归中的应用\" class=\"headerlink\" title=\"梯度下降算法在多元线性回归中的应用\"></a>梯度下降算法在多元线性回归中的应用</h2><p>对于假设函数：$h<em>θ (x)=θ^T x=θ_0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)}$<br>和损失函数：$J(θ_0,θ_1,…,θ_n)=\\frac{1}{2m} ∑</em>{i=1}^m(h_θ (x^{(i)} )−y^{(i)} )^2$<br>此时的梯度下降算法：<br>Repeat{</p>\n<script type=\"math/tex; mode=display\">\nθ_j≔θ_j−α\\frac{∂J(θ)}{∂θ_j}</script><p>}<br>对$\\frac{∂J(θ)}{∂θ_j}$进行等价变形：<br>Repeat{</p>\n<script type=\"math/tex; mode=display\">\nθ_j≔θ_j−α\\frac{1}{m}∑_{i=1}^m(h_θ (x^{(i)} )−y^{(i)})  x_j^i</script><p>}</p>\n"},{"_content":"<style>\nimg{\n    width: 40%;\n    padding-left: 20%;\n}\n</style>\n\n\n# 梯度下降算法\n\n在开始之前，为了方便解释，首先规定几个符号所代表的意义：\n$m$ 训练集中训练样本的数量\n$X$  输入变量\n$Y$  输出变量\n$(x,y)$ 训练样本\n$(x^i,y^i)$第i个训练样本（i表示一个索引）\n\n## 监督学习算法的流程\n\n提供训练集>学习算法得到$h$（假设函数：用于描绘x与y的关系）>预测y 的值\n\n## 代价/损失函数（Cost function）\n\n**假设函数(Hypothesis function)**——$h$是用来表示某一个数据集可能存在的线性/非线性关系的函数。对于线性拟合，其假设函数为：\n\n$$\nh_θ(x)=θ_1x+θ_0\n\n$$\n\n这其中的$θ$是假设函数当中的参数。\n也可以简化为：\n\n$$\nh_θ(x)=θ_1x$$  \n**代价函数**，在统计学上称为均方根误差函数。当假设函数中的系数$θ$取不同的值时，$\\frac{1}{2m}$倍假设函数预测值$h_θ(x^{(i)})$和真实值$y^{(i)}$的差的平方的和之间的函数关系表示为代价函数$J$。\n$$J(θ_0,θ_1)=\\frac{1}{2m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2$$ \n> 在这里取1/2的原因是便于消除求导之后产生的2倍,同时也可以进一步缩小$θ$  \n\n> 该函数的自变量是$θ_1$和$θ_0$，因此该函数是三维的函数（如图所示）。  \n ![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131130651.png)  \n\n**代价函数在几何上表示为数据集空间内的各点到假设函数的距离的平方的平均值的一半。**  \n要想使得数据能够被假设函数很好地拟合，那么代价函数要尽量地小。**当代价函数取到它的最小值即$J(θ_1)_{min}$时，此时的填入假设函数的$θ$对数据的拟合程度是最好的。**  \n对于线性的代价函数，假设函数对数据集的拟合程度越高，对应的$(θ_0,θ_1)$越接近代价函数图像等高线的中心。  \n![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131132852.png)   \n## 梯度下降算法（Gradient Descent）  \n### 梯度\n在微积分中，函数$f(x,y)$在$(x_0,y_0)$处是函数值增加最快的方向是**梯度（Gradient）**的方向，**梯度的反方向是函数值减小最快的方向。**  \n梯度的计算公式：  \n$$▿f|_{(x_0,y_0)}=(f_x(x_0,y_0),f_y(x_0,y_0))\n\n$$\n\n### 概述\n\n梯度下降算法是一种求解代价函数最小值的方法，它可以用在**多维任意的假设函数**当中。简而言之，梯度下降算法求得$J(θ_1)_{min}$的主要思路是：\n\n1. 给定$θ_0$和$θ_1$的初始值，通常令$θ_0=0$，$θ_1=0$。\n2. 不断改变$θ_0$和$θ_1$的值使得$J(θ_0,θ_1$的值逐渐变小，直到找到$J(θ_0,θ_1)$的最小值或者局部最小值。\n   如果从一个初始值出发，寻找附近的最小值，重复该过程，得到上图，最后得到的值为局部最优解。\n\n> 将梯度下降算法类比为爬山，从一个点开始，不断寻找“下山”的路线，最后找到一个“下山”的出口。——吴恩达\n\n当改变初始值时，会找到另一条“下山”的路径，找到第二个局部最优解（局部最小值）。\n![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131134419.png)\n对于线性回归的代价函数而言，只存在一个局部最小值。（见代价函数的图像）\n\n### 表示\n\n梯度下降算法可以表示为：\nRepeat untill convergence{\n\n$$\nθ_j:=θ_j-α\\frac{∂J(θ_0,θ_1)}{∂θ_j},j=0~and~j=1\n\n$$\n\n}解释：\n\n1. :=  表示赋值运算符\n2. α称为**学习率**，用来控制下降的**步长**（Padding），即更新的幅度：\n   如果α太小，同步更新的速率会非常的慢\n   而α过大，同步更新时可能会越过最小值点\n3. $\\frac{∂J(θ_0,θ_1)}{∂θ_j}$是代价函数的梯度：\n\n$$\n\\frac{∂J(θ_0,θ_1)}{∂θ_0}=\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})$$ \n$$\\frac{∂J(θ_0,θ_1)}{∂θ_1}=\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}$$   \n![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131135144.png)  \n△在代价函数中（以简化的代价函数为例），无论初始值在最小值点的左侧还是右侧，通过同步更新都能够使该点被“移动（Update）”到最小值，在最小值点，由于导数值为0，最终同步更新停止在了$θ_j=θ_j$，如前面所说，$θ_j$即为极小值点。  \n\n### 同步更新  \n**同步更新**（Simulaneous update）是实现梯度下降算法的最有效方式。  \n$$temp0:~~θ_0:=θ_0-α\\frac{∂J(θ_0,θ_1)}{∂θ_0}\n\n$$\n\n$$\ntemp1:~~θ_1:=θ_1-α\\frac{∂J(θ_0,θ_1)}{∂θ_1}\n\n$$\n\n$$\nθ_0:=temp0\n\n$$\n\n$$\nθ_1:=temp1\n\n$$\n\n这个更新方程能够同时更新$θ_0$和$θ_1$：$θ_0$更新为$θ_0-J'(θ)$，对$θ_1$同理。\n更新的方法是计算赋值号右边带入$θ_1$和$θ_2$的值进行计算，得到的两个值分别储存在temp0和temp1中，从上到下进行赋值。\n\n对于简化的代价函数：\n\n$$\nθ_1：=θ_1-αJ'(θ_1)$$  \n$$\\frac{dJ(θ_1,θ_0)}{dθ_j} =d(\\frac{1}{2m}Σ(h_θ(x_i)-y_i))^2)$$   \n\n\n将梯度代回代价函数中就得到了**Batch梯度下降法**的基本形式：  \nRepeat untill convergence{  \n$$θ_0:=θ_0-α\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})\n\n$$\n\n$$\nθ_1:=θ_1-α\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}\n\n$$\n\n}\n","source":"_posts/1. 线性回归/1.2. 梯度下降算法.md","raw":"<style>\nimg{\n    width: 40%;\n    padding-left: 20%;\n}\n</style>\n\n\n# 梯度下降算法\n\n在开始之前，为了方便解释，首先规定几个符号所代表的意义：\n$m$ 训练集中训练样本的数量\n$X$  输入变量\n$Y$  输出变量\n$(x,y)$ 训练样本\n$(x^i,y^i)$第i个训练样本（i表示一个索引）\n\n## 监督学习算法的流程\n\n提供训练集>学习算法得到$h$（假设函数：用于描绘x与y的关系）>预测y 的值\n\n## 代价/损失函数（Cost function）\n\n**假设函数(Hypothesis function)**——$h$是用来表示某一个数据集可能存在的线性/非线性关系的函数。对于线性拟合，其假设函数为：\n\n$$\nh_θ(x)=θ_1x+θ_0\n\n$$\n\n这其中的$θ$是假设函数当中的参数。\n也可以简化为：\n\n$$\nh_θ(x)=θ_1x$$  \n**代价函数**，在统计学上称为均方根误差函数。当假设函数中的系数$θ$取不同的值时，$\\frac{1}{2m}$倍假设函数预测值$h_θ(x^{(i)})$和真实值$y^{(i)}$的差的平方的和之间的函数关系表示为代价函数$J$。\n$$J(θ_0,θ_1)=\\frac{1}{2m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2$$ \n> 在这里取1/2的原因是便于消除求导之后产生的2倍,同时也可以进一步缩小$θ$  \n\n> 该函数的自变量是$θ_1$和$θ_0$，因此该函数是三维的函数（如图所示）。  \n ![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131130651.png)  \n\n**代价函数在几何上表示为数据集空间内的各点到假设函数的距离的平方的平均值的一半。**  \n要想使得数据能够被假设函数很好地拟合，那么代价函数要尽量地小。**当代价函数取到它的最小值即$J(θ_1)_{min}$时，此时的填入假设函数的$θ$对数据的拟合程度是最好的。**  \n对于线性的代价函数，假设函数对数据集的拟合程度越高，对应的$(θ_0,θ_1)$越接近代价函数图像等高线的中心。  \n![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131132852.png)   \n## 梯度下降算法（Gradient Descent）  \n### 梯度\n在微积分中，函数$f(x,y)$在$(x_0,y_0)$处是函数值增加最快的方向是**梯度（Gradient）**的方向，**梯度的反方向是函数值减小最快的方向。**  \n梯度的计算公式：  \n$$▿f|_{(x_0,y_0)}=(f_x(x_0,y_0),f_y(x_0,y_0))\n\n$$\n\n### 概述\n\n梯度下降算法是一种求解代价函数最小值的方法，它可以用在**多维任意的假设函数**当中。简而言之，梯度下降算法求得$J(θ_1)_{min}$的主要思路是：\n\n1. 给定$θ_0$和$θ_1$的初始值，通常令$θ_0=0$，$θ_1=0$。\n2. 不断改变$θ_0$和$θ_1$的值使得$J(θ_0,θ_1$的值逐渐变小，直到找到$J(θ_0,θ_1)$的最小值或者局部最小值。\n   如果从一个初始值出发，寻找附近的最小值，重复该过程，得到上图，最后得到的值为局部最优解。\n\n> 将梯度下降算法类比为爬山，从一个点开始，不断寻找“下山”的路线，最后找到一个“下山”的出口。——吴恩达\n\n当改变初始值时，会找到另一条“下山”的路径，找到第二个局部最优解（局部最小值）。\n![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131134419.png)\n对于线性回归的代价函数而言，只存在一个局部最小值。（见代价函数的图像）\n\n### 表示\n\n梯度下降算法可以表示为：\nRepeat untill convergence{\n\n$$\nθ_j:=θ_j-α\\frac{∂J(θ_0,θ_1)}{∂θ_j},j=0~and~j=1\n\n$$\n\n}解释：\n\n1. :=  表示赋值运算符\n2. α称为**学习率**，用来控制下降的**步长**（Padding），即更新的幅度：\n   如果α太小，同步更新的速率会非常的慢\n   而α过大，同步更新时可能会越过最小值点\n3. $\\frac{∂J(θ_0,θ_1)}{∂θ_j}$是代价函数的梯度：\n\n$$\n\\frac{∂J(θ_0,θ_1)}{∂θ_0}=\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})$$ \n$$\\frac{∂J(θ_0,θ_1)}{∂θ_1}=\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}$$   \n![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131135144.png)  \n△在代价函数中（以简化的代价函数为例），无论初始值在最小值点的左侧还是右侧，通过同步更新都能够使该点被“移动（Update）”到最小值，在最小值点，由于导数值为0，最终同步更新停止在了$θ_j=θ_j$，如前面所说，$θ_j$即为极小值点。  \n\n### 同步更新  \n**同步更新**（Simulaneous update）是实现梯度下降算法的最有效方式。  \n$$temp0:~~θ_0:=θ_0-α\\frac{∂J(θ_0,θ_1)}{∂θ_0}\n\n$$\n\n$$\ntemp1:~~θ_1:=θ_1-α\\frac{∂J(θ_0,θ_1)}{∂θ_1}\n\n$$\n\n$$\nθ_0:=temp0\n\n$$\n\n$$\nθ_1:=temp1\n\n$$\n\n这个更新方程能够同时更新$θ_0$和$θ_1$：$θ_0$更新为$θ_0-J'(θ)$，对$θ_1$同理。\n更新的方法是计算赋值号右边带入$θ_1$和$θ_2$的值进行计算，得到的两个值分别储存在temp0和temp1中，从上到下进行赋值。\n\n对于简化的代价函数：\n\n$$\nθ_1：=θ_1-αJ'(θ_1)$$  \n$$\\frac{dJ(θ_1,θ_0)}{dθ_j} =d(\\frac{1}{2m}Σ(h_θ(x_i)-y_i))^2)$$   \n\n\n将梯度代回代价函数中就得到了**Batch梯度下降法**的基本形式：  \nRepeat untill convergence{  \n$$θ_0:=θ_0-α\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})\n\n$$\n\n$$\nθ_1:=θ_1-α\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}\n\n$$\n\n}\n","slug":"1. 线性回归/1.2. 梯度下降算法","published":1,"date":"2021-02-06T03:54:46.298Z","updated":"2021-02-06T04:31:37.609Z","_id":"ckkt7gzvr0003880h479o21g2","title":"","comments":1,"layout":"post","photos":[],"link":"","content":"<style>\nimg{\n    width: 40%;\n    padding-left: 20%;\n}\n</style>\n\n\n<h1 id=\"梯度下降算法\"><a href=\"#梯度下降算法\" class=\"headerlink\" title=\"梯度下降算法\"></a>梯度下降算法</h1><p>在开始之前，为了方便解释，首先规定几个符号所代表的意义：<br>$m$ 训练集中训练样本的数量<br>$X$  输入变量<br>$Y$  输出变量<br>$(x,y)$ 训练样本<br>$(x^i,y^i)$第i个训练样本（i表示一个索引）</p>\n<h2 id=\"监督学习算法的流程\"><a href=\"#监督学习算法的流程\" class=\"headerlink\" title=\"监督学习算法的流程\"></a>监督学习算法的流程</h2><p>提供训练集&gt;学习算法得到$h$（假设函数：用于描绘x与y的关系）&gt;预测y 的值</p>\n<h2 id=\"代价-损失函数（Cost-function）\"><a href=\"#代价-损失函数（Cost-function）\" class=\"headerlink\" title=\"代价/损失函数（Cost function）\"></a>代价/损失函数（Cost function）</h2><p><strong>假设函数(Hypothesis function)</strong>——$h$是用来表示某一个数据集可能存在的线性/非线性关系的函数。对于线性拟合，其假设函数为：</p>\n<script type=\"math/tex; mode=display\">\nh_θ(x)=θ_1x+θ_0</script><p>这其中的$θ$是假设函数当中的参数。<br>也可以简化为：</p>\n<script type=\"math/tex; mode=display\">\nh_θ(x)=θ_1x</script><p><strong>代价函数</strong>，在统计学上称为均方根误差函数。当假设函数中的系数$θ$取不同的值时，$\\frac{1}{2m}$倍假设函数预测值$h_θ(x^{(i)})$和真实值$y^{(i)}$的差的平方的和之间的函数关系表示为代价函数$J$。</p>\n<script type=\"math/tex; mode=display\">J(θ_0,θ_1)=\\frac{1}{2m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2</script><blockquote>\n<p>在这里取1/2的原因是便于消除求导之后产生的2倍,同时也可以进一步缩小$θ$  </p>\n<p>该函数的自变量是$θ_1$和$θ_0$，因此该函数是三维的函数（如图所示）。<br> <img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131130651.png\" alt=\"\">  </p>\n</blockquote>\n<p><strong>代价函数在几何上表示为数据集空间内的各点到假设函数的距离的平方的平均值的一半。</strong><br>要想使得数据能够被假设函数很好地拟合，那么代价函数要尽量地小。<strong>当代价函数取到它的最小值即$J(θ<em>1)</em>{min}$时，此时的填入假设函数的$θ$对数据的拟合程度是最好的。</strong><br>对于线性的代价函数，假设函数对数据集的拟合程度越高，对应的$(θ_0,θ_1)$越接近代价函数图像等高线的中心。<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131132852.png\" alt=\"\">   </p>\n<h2 id=\"梯度下降算法（Gradient-Descent）\"><a href=\"#梯度下降算法（Gradient-Descent）\" class=\"headerlink\" title=\"梯度下降算法（Gradient Descent）\"></a>梯度下降算法（Gradient Descent）</h2><h3 id=\"梯度\"><a href=\"#梯度\" class=\"headerlink\" title=\"梯度\"></a>梯度</h3><p>在微积分中，函数$f(x,y)$在$(x_0,y_0)$处是函数值增加最快的方向是<strong>梯度（Gradient）</strong>的方向，<strong>梯度的反方向是函数值减小最快的方向。</strong><br>梯度的计算公式：  </p>\n<script type=\"math/tex; mode=display\">▿f|_{(x_0,y_0)}=(f_x(x_0,y_0),f_y(x_0,y_0))</script><h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>梯度下降算法是一种求解代价函数最小值的方法，它可以用在<strong>多维任意的假设函数</strong>当中。简而言之，梯度下降算法求得$J(θ<em>1)</em>{min}$的主要思路是：</p>\n<ol>\n<li>给定$θ_0$和$θ_1$的初始值，通常令$θ_0=0$，$θ_1=0$。</li>\n<li>不断改变$θ_0$和$θ_1$的值使得$J(θ_0,θ_1$的值逐渐变小，直到找到$J(θ_0,θ_1)$的最小值或者局部最小值。<br>如果从一个初始值出发，寻找附近的最小值，重复该过程，得到上图，最后得到的值为局部最优解。</li>\n</ol>\n<blockquote>\n<p>将梯度下降算法类比为爬山，从一个点开始，不断寻找“下山”的路线，最后找到一个“下山”的出口。——吴恩达</p>\n</blockquote>\n<p>当改变初始值时，会找到另一条“下山”的路径，找到第二个局部最优解（局部最小值）。<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131134419.png\" alt=\"\"><br>对于线性回归的代价函数而言，只存在一个局部最小值。（见代价函数的图像）</p>\n<h3 id=\"表示\"><a href=\"#表示\" class=\"headerlink\" title=\"表示\"></a>表示</h3><p>梯度下降算法可以表示为：<br>Repeat untill convergence{</p>\n<script type=\"math/tex; mode=display\">\nθ_j:=θ_j-α\\frac{∂J(θ_0,θ_1)}{∂θ_j},j=0~and~j=1</script><p>}解释：</p>\n<ol>\n<li>:=  表示赋值运算符</li>\n<li>α称为<strong>学习率</strong>，用来控制下降的<strong>步长</strong>（Padding），即更新的幅度：<br>如果α太小，同步更新的速率会非常的慢<br>而α过大，同步更新时可能会越过最小值点</li>\n<li>$\\frac{∂J(θ_0,θ_1)}{∂θ_j}$是代价函数的梯度：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\frac{∂J(θ_0,θ_1)}{∂θ_0}=\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})</script><script type=\"math/tex; mode=display\">\\frac{∂J(θ_0,θ_1)}{∂θ_1}=\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}</script><p><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131135144.png\" alt=\"\"><br>△在代价函数中（以简化的代价函数为例），无论初始值在最小值点的左侧还是右侧，通过同步更新都能够使该点被“移动（Update）”到最小值，在最小值点，由于导数值为0，最终同步更新停止在了$θ_j=θ_j$，如前面所说，$θ_j$即为极小值点。  </p>\n<h3 id=\"同步更新\"><a href=\"#同步更新\" class=\"headerlink\" title=\"同步更新\"></a>同步更新</h3><p><strong>同步更新</strong>（Simulaneous update）是实现梯度下降算法的最有效方式。  </p>\n<script type=\"math/tex; mode=display\">temp0:~~θ_0:=θ_0-α\\frac{∂J(θ_0,θ_1)}{∂θ_0}</script><script type=\"math/tex; mode=display\">\ntemp1:~~θ_1:=θ_1-α\\frac{∂J(θ_0,θ_1)}{∂θ_1}</script><script type=\"math/tex; mode=display\">\nθ_0:=temp0</script><script type=\"math/tex; mode=display\">\nθ_1:=temp1</script><p>这个更新方程能够同时更新$θ_0$和$θ_1$：$θ_0$更新为$θ_0-J’(θ)$，对$θ_1$同理。<br>更新的方法是计算赋值号右边带入$θ_1$和$θ_2$的值进行计算，得到的两个值分别储存在temp0和temp1中，从上到下进行赋值。</p>\n<p>对于简化的代价函数：</p>\n<script type=\"math/tex; mode=display\">\nθ_1：=θ_1-αJ'(θ_1)</script><script type=\"math/tex; mode=display\">\\frac{dJ(θ_1,θ_0)}{dθ_j} =d(\\frac{1}{2m}Σ(h_θ(x_i)-y_i))^2)</script><p>将梯度代回代价函数中就得到了<strong>Batch梯度下降法</strong>的基本形式：<br>Repeat untill convergence{  </p>\n<script type=\"math/tex; mode=display\">θ_0:=θ_0-α\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})</script><script type=\"math/tex; mode=display\">\nθ_1:=θ_1-α\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}</script><p>}</p>\n","site":{"data":{}},"excerpt":"","more":"<style>\nimg{\n    width: 40%;\n    padding-left: 20%;\n}\n</style>\n\n\n<h1 id=\"梯度下降算法\"><a href=\"#梯度下降算法\" class=\"headerlink\" title=\"梯度下降算法\"></a>梯度下降算法</h1><p>在开始之前，为了方便解释，首先规定几个符号所代表的意义：<br>$m$ 训练集中训练样本的数量<br>$X$  输入变量<br>$Y$  输出变量<br>$(x,y)$ 训练样本<br>$(x^i,y^i)$第i个训练样本（i表示一个索引）</p>\n<h2 id=\"监督学习算法的流程\"><a href=\"#监督学习算法的流程\" class=\"headerlink\" title=\"监督学习算法的流程\"></a>监督学习算法的流程</h2><p>提供训练集&gt;学习算法得到$h$（假设函数：用于描绘x与y的关系）&gt;预测y 的值</p>\n<h2 id=\"代价-损失函数（Cost-function）\"><a href=\"#代价-损失函数（Cost-function）\" class=\"headerlink\" title=\"代价/损失函数（Cost function）\"></a>代价/损失函数（Cost function）</h2><p><strong>假设函数(Hypothesis function)</strong>——$h$是用来表示某一个数据集可能存在的线性/非线性关系的函数。对于线性拟合，其假设函数为：</p>\n<script type=\"math/tex; mode=display\">\nh_θ(x)=θ_1x+θ_0</script><p>这其中的$θ$是假设函数当中的参数。<br>也可以简化为：</p>\n<script type=\"math/tex; mode=display\">\nh_θ(x)=θ_1x</script><p><strong>代价函数</strong>，在统计学上称为均方根误差函数。当假设函数中的系数$θ$取不同的值时，$\\frac{1}{2m}$倍假设函数预测值$h_θ(x^{(i)})$和真实值$y^{(i)}$的差的平方的和之间的函数关系表示为代价函数$J$。</p>\n<script type=\"math/tex; mode=display\">J(θ_0,θ_1)=\\frac{1}{2m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2</script><blockquote>\n<p>在这里取1/2的原因是便于消除求导之后产生的2倍,同时也可以进一步缩小$θ$  </p>\n<p>该函数的自变量是$θ_1$和$θ_0$，因此该函数是三维的函数（如图所示）。<br> <img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131130651.png\" alt=\"\">  </p>\n</blockquote>\n<p><strong>代价函数在几何上表示为数据集空间内的各点到假设函数的距离的平方的平均值的一半。</strong><br>要想使得数据能够被假设函数很好地拟合，那么代价函数要尽量地小。<strong>当代价函数取到它的最小值即$J(θ<em>1)</em>{min}$时，此时的填入假设函数的$θ$对数据的拟合程度是最好的。</strong><br>对于线性的代价函数，假设函数对数据集的拟合程度越高，对应的$(θ_0,θ_1)$越接近代价函数图像等高线的中心。<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131132852.png\" alt=\"\">   </p>\n<h2 id=\"梯度下降算法（Gradient-Descent）\"><a href=\"#梯度下降算法（Gradient-Descent）\" class=\"headerlink\" title=\"梯度下降算法（Gradient Descent）\"></a>梯度下降算法（Gradient Descent）</h2><h3 id=\"梯度\"><a href=\"#梯度\" class=\"headerlink\" title=\"梯度\"></a>梯度</h3><p>在微积分中，函数$f(x,y)$在$(x_0,y_0)$处是函数值增加最快的方向是<strong>梯度（Gradient）</strong>的方向，<strong>梯度的反方向是函数值减小最快的方向。</strong><br>梯度的计算公式：  </p>\n<script type=\"math/tex; mode=display\">▿f|_{(x_0,y_0)}=(f_x(x_0,y_0),f_y(x_0,y_0))</script><h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>梯度下降算法是一种求解代价函数最小值的方法，它可以用在<strong>多维任意的假设函数</strong>当中。简而言之，梯度下降算法求得$J(θ<em>1)</em>{min}$的主要思路是：</p>\n<ol>\n<li>给定$θ_0$和$θ_1$的初始值，通常令$θ_0=0$，$θ_1=0$。</li>\n<li>不断改变$θ_0$和$θ_1$的值使得$J(θ_0,θ_1$的值逐渐变小，直到找到$J(θ_0,θ_1)$的最小值或者局部最小值。<br>如果从一个初始值出发，寻找附近的最小值，重复该过程，得到上图，最后得到的值为局部最优解。</li>\n</ol>\n<blockquote>\n<p>将梯度下降算法类比为爬山，从一个点开始，不断寻找“下山”的路线，最后找到一个“下山”的出口。——吴恩达</p>\n</blockquote>\n<p>当改变初始值时，会找到另一条“下山”的路径，找到第二个局部最优解（局部最小值）。<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131134419.png\" alt=\"\"><br>对于线性回归的代价函数而言，只存在一个局部最小值。（见代价函数的图像）</p>\n<h3 id=\"表示\"><a href=\"#表示\" class=\"headerlink\" title=\"表示\"></a>表示</h3><p>梯度下降算法可以表示为：<br>Repeat untill convergence{</p>\n<script type=\"math/tex; mode=display\">\nθ_j:=θ_j-α\\frac{∂J(θ_0,θ_1)}{∂θ_j},j=0~and~j=1</script><p>}解释：</p>\n<ol>\n<li>:=  表示赋值运算符</li>\n<li>α称为<strong>学习率</strong>，用来控制下降的<strong>步长</strong>（Padding），即更新的幅度：<br>如果α太小，同步更新的速率会非常的慢<br>而α过大，同步更新时可能会越过最小值点</li>\n<li>$\\frac{∂J(θ_0,θ_1)}{∂θ_j}$是代价函数的梯度：</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\frac{∂J(θ_0,θ_1)}{∂θ_0}=\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})</script><script type=\"math/tex; mode=display\">\\frac{∂J(θ_0,θ_1)}{∂θ_1}=\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}</script><p><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131135144.png\" alt=\"\"><br>△在代价函数中（以简化的代价函数为例），无论初始值在最小值点的左侧还是右侧，通过同步更新都能够使该点被“移动（Update）”到最小值，在最小值点，由于导数值为0，最终同步更新停止在了$θ_j=θ_j$，如前面所说，$θ_j$即为极小值点。  </p>\n<h3 id=\"同步更新\"><a href=\"#同步更新\" class=\"headerlink\" title=\"同步更新\"></a>同步更新</h3><p><strong>同步更新</strong>（Simulaneous update）是实现梯度下降算法的最有效方式。  </p>\n<script type=\"math/tex; mode=display\">temp0:~~θ_0:=θ_0-α\\frac{∂J(θ_0,θ_1)}{∂θ_0}</script><script type=\"math/tex; mode=display\">\ntemp1:~~θ_1:=θ_1-α\\frac{∂J(θ_0,θ_1)}{∂θ_1}</script><script type=\"math/tex; mode=display\">\nθ_0:=temp0</script><script type=\"math/tex; mode=display\">\nθ_1:=temp1</script><p>这个更新方程能够同时更新$θ_0$和$θ_1$：$θ_0$更新为$θ_0-J’(θ)$，对$θ_1$同理。<br>更新的方法是计算赋值号右边带入$θ_1$和$θ_2$的值进行计算，得到的两个值分别储存在temp0和temp1中，从上到下进行赋值。</p>\n<p>对于简化的代价函数：</p>\n<script type=\"math/tex; mode=display\">\nθ_1：=θ_1-αJ'(θ_1)</script><script type=\"math/tex; mode=display\">\\frac{dJ(θ_1,θ_0)}{dθ_j} =d(\\frac{1}{2m}Σ(h_θ(x_i)-y_i))^2)</script><p>将梯度代回代价函数中就得到了<strong>Batch梯度下降法</strong>的基本形式：<br>Repeat untill convergence{  </p>\n<script type=\"math/tex; mode=display\">θ_0:=θ_0-α\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})</script><script type=\"math/tex; mode=display\">\nθ_1:=θ_1-α\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}</script><p>}</p>\n"},{"title":"1.5. 多项式拟合和正规方程","math":true,"_content":"\n# 多项式拟合和正规方程\n\n## 特征点的创建和合并\n\n对于一个特定的问题，可以产生不同的特征点，**通过对问题参数的重新定义和对原有特征点的数学处理合并拆分，能够得到更加优秀的特征点。**\n\n## 多项式回归\n\n对于更多更加常见的数学模型，其拟合往往是非线性关系的，这时候就需要考虑引用多项式来进行拟合，如：$h(x)=θ_0+θ_1 x+θ_2 x^2+θ_3 x^3$\n\n## 正规方程算法\n\n在微积分中，对于函数$f(x,y)$，其局部最值往往是在$f_x=0$ 且$f_y=0$处取得。\n因此，对于代价函数$J(θ)$，求$J(θ)$对每一个$θ_i$的偏导数，令它们都为0，即：\n\n$$\n\\frac{∂J(θ)}{∂θ_i}=0~for~i=0,1,2,…,n\n\n$$\n\n称为**正规方程**(Regular expression)。正规方程提供了一种直接求出最小值的方法，而不需要依赖迭代进行一步一步地运算。\n\n### 正规方程的矩阵形式\n\n对于数据集$\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\\}$,  其中每一个$x^{(i)}$都是一个向量：$x^{(i)}=\\begin{bmatrix}x_0^{(i)}\\\\x_1^{(i)}\\\\...\\\\x_n^{(i)}\\end{bmatrix}$\n构建**设计矩阵**（Design matrix）$X=\\begin{bmatrix}(x^{(1)})^T\\\\(x^{(2})^T\\\\...\\\\(x^{(m)})^T\\end{bmatrix}$和值向量$y=\\begin{bmatrix}  y^{(1)}\\\\y^{(2)}\\\\...\\\\y^{(m)}  \\end{bmatrix}$\n将代价函数转化为矩阵方程的形式，再对其求导，令其等于0，得到代价函数取得最小值时的$θ$：\n\n$$\nθ=(X^TX)^{-1}X^Ty\n\n$$\n\n对比梯度下降算法：\n正规方程算法不需要学习率和迭代，但**对大规模数量（万数量级以上）的特征点（n），工作效率十分低下**。对于一些如分类算法等等更加复杂的算法，正规方程法并不适用于求它们在极值处的θ值。\n\n### 正规方程的不可逆性\n\n在使用正规方程时，要注意的问题是，**如果设计矩阵X不可逆（为奇异矩阵），正规方程会无法使用。**\n\n设计矩阵为奇异矩阵的常见情况：\n\n1. x-I 不满足线性关系\n2. 正在运行的学习算法中，特征点的数量大于样本点的数量（使得$m≤n$）\n\n当设计矩阵X不可逆时，应当尝试删除一些特征点，或者考虑正规化（Regularation）。\n但是总体而言，**矩阵X不可逆的情况是极少数的。**\n","source":"_posts/1. 线性回归/1.5. 多项式拟合和正规方程.md","raw":"---\ntitle: 1.5. 多项式拟合和正规方程\nmath: true\n\n---\n\n# 多项式拟合和正规方程\n\n## 特征点的创建和合并\n\n对于一个特定的问题，可以产生不同的特征点，**通过对问题参数的重新定义和对原有特征点的数学处理合并拆分，能够得到更加优秀的特征点。**\n\n## 多项式回归\n\n对于更多更加常见的数学模型，其拟合往往是非线性关系的，这时候就需要考虑引用多项式来进行拟合，如：$h(x)=θ_0+θ_1 x+θ_2 x^2+θ_3 x^3$\n\n## 正规方程算法\n\n在微积分中，对于函数$f(x,y)$，其局部最值往往是在$f_x=0$ 且$f_y=0$处取得。\n因此，对于代价函数$J(θ)$，求$J(θ)$对每一个$θ_i$的偏导数，令它们都为0，即：\n\n$$\n\\frac{∂J(θ)}{∂θ_i}=0~for~i=0,1,2,…,n\n\n$$\n\n称为**正规方程**(Regular expression)。正规方程提供了一种直接求出最小值的方法，而不需要依赖迭代进行一步一步地运算。\n\n### 正规方程的矩阵形式\n\n对于数据集$\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\\}$,  其中每一个$x^{(i)}$都是一个向量：$x^{(i)}=\\begin{bmatrix}x_0^{(i)}\\\\x_1^{(i)}\\\\...\\\\x_n^{(i)}\\end{bmatrix}$\n构建**设计矩阵**（Design matrix）$X=\\begin{bmatrix}(x^{(1)})^T\\\\(x^{(2})^T\\\\...\\\\(x^{(m)})^T\\end{bmatrix}$和值向量$y=\\begin{bmatrix}  y^{(1)}\\\\y^{(2)}\\\\...\\\\y^{(m)}  \\end{bmatrix}$\n将代价函数转化为矩阵方程的形式，再对其求导，令其等于0，得到代价函数取得最小值时的$θ$：\n\n$$\nθ=(X^TX)^{-1}X^Ty\n\n$$\n\n对比梯度下降算法：\n正规方程算法不需要学习率和迭代，但**对大规模数量（万数量级以上）的特征点（n），工作效率十分低下**。对于一些如分类算法等等更加复杂的算法，正规方程法并不适用于求它们在极值处的θ值。\n\n### 正规方程的不可逆性\n\n在使用正规方程时，要注意的问题是，**如果设计矩阵X不可逆（为奇异矩阵），正规方程会无法使用。**\n\n设计矩阵为奇异矩阵的常见情况：\n\n1. x-I 不满足线性关系\n2. 正在运行的学习算法中，特征点的数量大于样本点的数量（使得$m≤n$）\n\n当设计矩阵X不可逆时，应当尝试删除一些特征点，或者考虑正规化（Regularation）。\n但是总体而言，**矩阵X不可逆的情况是极少数的。**\n","slug":"1. 线性回归/1.5. 多项式拟合和正规方程","published":1,"date":"2021-02-06T03:54:46.306Z","updated":"2021-02-06T04:19:12.702Z","_id":"ckkt7gzvs0004880h6jrr6mew","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"多项式拟合和正规方程\"><a href=\"#多项式拟合和正规方程\" class=\"headerlink\" title=\"多项式拟合和正规方程\"></a>多项式拟合和正规方程</h1><h2 id=\"特征点的创建和合并\"><a href=\"#特征点的创建和合并\" class=\"headerlink\" title=\"特征点的创建和合并\"></a>特征点的创建和合并</h2><p>对于一个特定的问题，可以产生不同的特征点，<strong>通过对问题参数的重新定义和对原有特征点的数学处理合并拆分，能够得到更加优秀的特征点。</strong></p>\n<h2 id=\"多项式回归\"><a href=\"#多项式回归\" class=\"headerlink\" title=\"多项式回归\"></a>多项式回归</h2><p>对于更多更加常见的数学模型，其拟合往往是非线性关系的，这时候就需要考虑引用多项式来进行拟合，如：$h(x)=θ_0+θ_1 x+θ_2 x^2+θ_3 x^3$</p>\n<h2 id=\"正规方程算法\"><a href=\"#正规方程算法\" class=\"headerlink\" title=\"正规方程算法\"></a>正规方程算法</h2><p>在微积分中，对于函数$f(x,y)$，其局部最值往往是在$f_x=0$ 且$f_y=0$处取得。<br>因此，对于代价函数$J(θ)$，求$J(θ)$对每一个$θ_i$的偏导数，令它们都为0，即：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{∂J(θ)}{∂θ_i}=0~for~i=0,1,2,…,n</script><p>称为<strong>正规方程</strong>(Regular expression)。正规方程提供了一种直接求出最小值的方法，而不需要依赖迭代进行一步一步地运算。</p>\n<h3 id=\"正规方程的矩阵形式\"><a href=\"#正规方程的矩阵形式\" class=\"headerlink\" title=\"正规方程的矩阵形式\"></a>正规方程的矩阵形式</h3><p>对于数据集${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$,  其中每一个$x^{(i)}$都是一个向量：$x^{(i)}=\\begin{bmatrix}x_0^{(i)}\\x_1^{(i)}\\…\\x_n^{(i)}\\end{bmatrix}$<br>构建<strong>设计矩阵</strong>（Design matrix）$X=\\begin{bmatrix}(x^{(1)})^T\\(x^{(2})^T\\…\\(x^{(m)})^T\\end{bmatrix}$和值向量$y=\\begin{bmatrix}  y^{(1)}\\y^{(2)}\\…\\y^{(m)}  \\end{bmatrix}$<br>将代价函数转化为矩阵方程的形式，再对其求导，令其等于0，得到代价函数取得最小值时的$θ$：</p>\n<script type=\"math/tex; mode=display\">\nθ=(X^TX)^{-1}X^Ty</script><p>对比梯度下降算法：<br>正规方程算法不需要学习率和迭代，但<strong>对大规模数量（万数量级以上）的特征点（n），工作效率十分低下</strong>。对于一些如分类算法等等更加复杂的算法，正规方程法并不适用于求它们在极值处的θ值。</p>\n<h3 id=\"正规方程的不可逆性\"><a href=\"#正规方程的不可逆性\" class=\"headerlink\" title=\"正规方程的不可逆性\"></a>正规方程的不可逆性</h3><p>在使用正规方程时，要注意的问题是，<strong>如果设计矩阵X不可逆（为奇异矩阵），正规方程会无法使用。</strong></p>\n<p>设计矩阵为奇异矩阵的常见情况：</p>\n<ol>\n<li>x-I 不满足线性关系</li>\n<li>正在运行的学习算法中，特征点的数量大于样本点的数量（使得$m≤n$）</li>\n</ol>\n<p>当设计矩阵X不可逆时，应当尝试删除一些特征点，或者考虑正规化（Regularation）。<br>但是总体而言，<strong>矩阵X不可逆的情况是极少数的。</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"多项式拟合和正规方程\"><a href=\"#多项式拟合和正规方程\" class=\"headerlink\" title=\"多项式拟合和正规方程\"></a>多项式拟合和正规方程</h1><h2 id=\"特征点的创建和合并\"><a href=\"#特征点的创建和合并\" class=\"headerlink\" title=\"特征点的创建和合并\"></a>特征点的创建和合并</h2><p>对于一个特定的问题，可以产生不同的特征点，<strong>通过对问题参数的重新定义和对原有特征点的数学处理合并拆分，能够得到更加优秀的特征点。</strong></p>\n<h2 id=\"多项式回归\"><a href=\"#多项式回归\" class=\"headerlink\" title=\"多项式回归\"></a>多项式回归</h2><p>对于更多更加常见的数学模型，其拟合往往是非线性关系的，这时候就需要考虑引用多项式来进行拟合，如：$h(x)=θ_0+θ_1 x+θ_2 x^2+θ_3 x^3$</p>\n<h2 id=\"正规方程算法\"><a href=\"#正规方程算法\" class=\"headerlink\" title=\"正规方程算法\"></a>正规方程算法</h2><p>在微积分中，对于函数$f(x,y)$，其局部最值往往是在$f_x=0$ 且$f_y=0$处取得。<br>因此，对于代价函数$J(θ)$，求$J(θ)$对每一个$θ_i$的偏导数，令它们都为0，即：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{∂J(θ)}{∂θ_i}=0~for~i=0,1,2,…,n</script><p>称为<strong>正规方程</strong>(Regular expression)。正规方程提供了一种直接求出最小值的方法，而不需要依赖迭代进行一步一步地运算。</p>\n<h3 id=\"正规方程的矩阵形式\"><a href=\"#正规方程的矩阵形式\" class=\"headerlink\" title=\"正规方程的矩阵形式\"></a>正规方程的矩阵形式</h3><p>对于数据集${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$,  其中每一个$x^{(i)}$都是一个向量：$x^{(i)}=\\begin{bmatrix}x_0^{(i)}\\x_1^{(i)}\\…\\x_n^{(i)}\\end{bmatrix}$<br>构建<strong>设计矩阵</strong>（Design matrix）$X=\\begin{bmatrix}(x^{(1)})^T\\(x^{(2})^T\\…\\(x^{(m)})^T\\end{bmatrix}$和值向量$y=\\begin{bmatrix}  y^{(1)}\\y^{(2)}\\…\\y^{(m)}  \\end{bmatrix}$<br>将代价函数转化为矩阵方程的形式，再对其求导，令其等于0，得到代价函数取得最小值时的$θ$：</p>\n<script type=\"math/tex; mode=display\">\nθ=(X^TX)^{-1}X^Ty</script><p>对比梯度下降算法：<br>正规方程算法不需要学习率和迭代，但<strong>对大规模数量（万数量级以上）的特征点（n），工作效率十分低下</strong>。对于一些如分类算法等等更加复杂的算法，正规方程法并不适用于求它们在极值处的θ值。</p>\n<h3 id=\"正规方程的不可逆性\"><a href=\"#正规方程的不可逆性\" class=\"headerlink\" title=\"正规方程的不可逆性\"></a>正规方程的不可逆性</h3><p>在使用正规方程时，要注意的问题是，<strong>如果设计矩阵X不可逆（为奇异矩阵），正规方程会无法使用。</strong></p>\n<p>设计矩阵为奇异矩阵的常见情况：</p>\n<ol>\n<li>x-I 不满足线性关系</li>\n<li>正在运行的学习算法中，特征点的数量大于样本点的数量（使得$m≤n$）</li>\n</ol>\n<p>当设计矩阵X不可逆时，应当尝试删除一些特征点，或者考虑正规化（Regularation）。<br>但是总体而言，<strong>矩阵X不可逆的情况是极少数的。</strong></p>\n"},{"title":"1.4. 调试方法","math":true,"_content":"\n# 调试方法\n\n## 特征缩放\n\n对于某些不具有比较性的样本特征$x_i$ （比如对其他的x来说$x_i$ 相当大或者相当小），梯度下降的过程可能会非常漫长，并且可能来回波动才能最后收敛到全局的最小值。\n![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144432.png)\n在这样的情况下，可以对$x_i$ 进行缩放（如 $x_i≔αx_i$  或者 $x_i=x_i/α$），使得$x_i$ 与其他的$x$具有可比性，以增加梯度下降的效率。\n**通常将$x$缩放至⟦-1,1⟧**的区间内。（只表示一个大致的范围，这不是绝对的。）\n\n## 均值归一\n\n将$x_i$  替换为$x_i−μ_i$ 使得特征值具有为0的平均值（对$x_0$ 不适用）\n\n$$\nx_i:=(x_i−μ_i)/s_i\n\n$$\n\n定义$μ_i$  为训练集$X$ 的平均值，$s_i=|x_imax−x_imin |$, 表示$x_i$ 的取值范围（近似值），或者直接设置为$s_i$ 的标准差。\n\n## 学习率(Learning rate)\n\n梯度下降调试的方法：\n\n1. 绘制$minJ(θ)-batch$的图像\n   ![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144740.png)\n   原则：每一个batch之后 θ 的值都应该减小，这样的图像能够通过直观地表现变化率来表现梯度下降是否收敛（变化率为0）。\n2. 自动收敛测试\n   如果$J(θ)$在某一次迭代之后的下降值小于某个值$ε$后，就能够判断算法已经达到了收敛。\n   $ε$的值比较难取，所以通常采取1.中的方法进行观测。\n\n常见的α过大的$minJ(θ)-batch$的图像：\nα过大,导致代价函数无法收敛\n![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144816.png)\n\nα过小，导致代价函数收敛速度过慢\n\n$$\n\n\n\n$$\n","source":"_posts/1. 线性回归/1.4. 调试方法.md","raw":"---\ntitle: 1.4. 调试方法\nmath: true\n\n---\n\n# 调试方法\n\n## 特征缩放\n\n对于某些不具有比较性的样本特征$x_i$ （比如对其他的x来说$x_i$ 相当大或者相当小），梯度下降的过程可能会非常漫长，并且可能来回波动才能最后收敛到全局的最小值。\n![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144432.png)\n在这样的情况下，可以对$x_i$ 进行缩放（如 $x_i≔αx_i$  或者 $x_i=x_i/α$），使得$x_i$ 与其他的$x$具有可比性，以增加梯度下降的效率。\n**通常将$x$缩放至⟦-1,1⟧**的区间内。（只表示一个大致的范围，这不是绝对的。）\n\n## 均值归一\n\n将$x_i$  替换为$x_i−μ_i$ 使得特征值具有为0的平均值（对$x_0$ 不适用）\n\n$$\nx_i:=(x_i−μ_i)/s_i\n\n$$\n\n定义$μ_i$  为训练集$X$ 的平均值，$s_i=|x_imax−x_imin |$, 表示$x_i$ 的取值范围（近似值），或者直接设置为$s_i$ 的标准差。\n\n## 学习率(Learning rate)\n\n梯度下降调试的方法：\n\n1. 绘制$minJ(θ)-batch$的图像\n   ![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144740.png)\n   原则：每一个batch之后 θ 的值都应该减小，这样的图像能够通过直观地表现变化率来表现梯度下降是否收敛（变化率为0）。\n2. 自动收敛测试\n   如果$J(θ)$在某一次迭代之后的下降值小于某个值$ε$后，就能够判断算法已经达到了收敛。\n   $ε$的值比较难取，所以通常采取1.中的方法进行观测。\n\n常见的α过大的$minJ(θ)-batch$的图像：\nα过大,导致代价函数无法收敛\n![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144816.png)\n\nα过小，导致代价函数收敛速度过慢\n\n$$\n\n\n\n$$\n","slug":"1. 线性回归/1.4. 调试方法","published":1,"date":"2021-02-06T03:54:46.303Z","updated":"2021-02-06T04:19:04.268Z","_id":"ckkt7gzvt0005880haaxb3bxb","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"调试方法\"><a href=\"#调试方法\" class=\"headerlink\" title=\"调试方法\"></a>调试方法</h1><h2 id=\"特征缩放\"><a href=\"#特征缩放\" class=\"headerlink\" title=\"特征缩放\"></a>特征缩放</h2><p>对于某些不具有比较性的样本特征$x_i$ （比如对其他的x来说$x_i$ 相当大或者相当小），梯度下降的过程可能会非常漫长，并且可能来回波动才能最后收敛到全局的最小值。<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144432.png\" alt=\"\"><br>在这样的情况下，可以对$x_i$ 进行缩放（如 $x_i≔αx_i$  或者 $x_i=x_i/α$），使得$x_i$ 与其他的$x$具有可比性，以增加梯度下降的效率。<br><strong>通常将$x$缩放至⟦-1,1⟧</strong>的区间内。（只表示一个大致的范围，这不是绝对的。）</p>\n<h2 id=\"均值归一\"><a href=\"#均值归一\" class=\"headerlink\" title=\"均值归一\"></a>均值归一</h2><p>将$x_i$  替换为$x_i−μ_i$ 使得特征值具有为0的平均值（对$x_0$ 不适用）</p>\n<script type=\"math/tex; mode=display\">\nx_i:=(x_i−μ_i)/s_i</script><p>定义$μ_i$  为训练集$X$ 的平均值，$s_i=|x_imax−x_imin |$, 表示$x_i$ 的取值范围（近似值），或者直接设置为$s_i$ 的标准差。</p>\n<h2 id=\"学习率-Learning-rate\"><a href=\"#学习率-Learning-rate\" class=\"headerlink\" title=\"学习率(Learning rate)\"></a>学习率(Learning rate)</h2><p>梯度下降调试的方法：</p>\n<ol>\n<li>绘制$minJ(θ)-batch$的图像<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144740.png\" alt=\"\"><br>原则：每一个batch之后 θ 的值都应该减小，这样的图像能够通过直观地表现变化率来表现梯度下降是否收敛（变化率为0）。</li>\n<li>自动收敛测试<br>如果$J(θ)$在某一次迭代之后的下降值小于某个值$ε$后，就能够判断算法已经达到了收敛。<br>$ε$的值比较难取，所以通常采取1.中的方法进行观测。</li>\n</ol>\n<p>常见的α过大的$minJ(θ)-batch$的图像：<br>α过大,导致代价函数无法收敛<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144816.png\" alt=\"\"></p>\n<p>α过小，导致代价函数收敛速度过慢</p>\n<script type=\"math/tex; mode=display\">\n</script>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"调试方法\"><a href=\"#调试方法\" class=\"headerlink\" title=\"调试方法\"></a>调试方法</h1><h2 id=\"特征缩放\"><a href=\"#特征缩放\" class=\"headerlink\" title=\"特征缩放\"></a>特征缩放</h2><p>对于某些不具有比较性的样本特征$x_i$ （比如对其他的x来说$x_i$ 相当大或者相当小），梯度下降的过程可能会非常漫长，并且可能来回波动才能最后收敛到全局的最小值。<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144432.png\" alt=\"\"><br>在这样的情况下，可以对$x_i$ 进行缩放（如 $x_i≔αx_i$  或者 $x_i=x_i/α$），使得$x_i$ 与其他的$x$具有可比性，以增加梯度下降的效率。<br><strong>通常将$x$缩放至⟦-1,1⟧</strong>的区间内。（只表示一个大致的范围，这不是绝对的。）</p>\n<h2 id=\"均值归一\"><a href=\"#均值归一\" class=\"headerlink\" title=\"均值归一\"></a>均值归一</h2><p>将$x_i$  替换为$x_i−μ_i$ 使得特征值具有为0的平均值（对$x_0$ 不适用）</p>\n<script type=\"math/tex; mode=display\">\nx_i:=(x_i−μ_i)/s_i</script><p>定义$μ_i$  为训练集$X$ 的平均值，$s_i=|x_imax−x_imin |$, 表示$x_i$ 的取值范围（近似值），或者直接设置为$s_i$ 的标准差。</p>\n<h2 id=\"学习率-Learning-rate\"><a href=\"#学习率-Learning-rate\" class=\"headerlink\" title=\"学习率(Learning rate)\"></a>学习率(Learning rate)</h2><p>梯度下降调试的方法：</p>\n<ol>\n<li>绘制$minJ(θ)-batch$的图像<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144740.png\" alt=\"\"><br>原则：每一个batch之后 θ 的值都应该减小，这样的图像能够通过直观地表现变化率来表现梯度下降是否收敛（变化率为0）。</li>\n<li>自动收敛测试<br>如果$J(θ)$在某一次迭代之后的下降值小于某个值$ε$后，就能够判断算法已经达到了收敛。<br>$ε$的值比较难取，所以通常采取1.中的方法进行观测。</li>\n</ol>\n<p>常见的α过大的$minJ(θ)-batch$的图像：<br>α过大,导致代价函数无法收敛<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144816.png\" alt=\"\"></p>\n<p>α过小，导致代价函数收敛速度过慢</p>\n<script type=\"math/tex; mode=display\">\n</script>"},{"title":"1.1. 什么是机器学习","math":true,"_content":"\n# 什么是机器学习\n\n## 机器学习的定义\n\n> A computer  program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.    ——Tom Mitchelle\n\n简言之，机器学习通过完成任务（T）得到经验（E），进而提升性能（P）。\n例如：一个自我对弈的跳棋学习机器：E ：自我对弈的棋局 T：下跳棋 P：与新对手玩跳棋时的获胜概率\n\n## 机器学习的主要算法类型\n\n- **监督学习**（Supervised）人教会计算机完成任务。根据统计数据做直线或曲线拟合/分离数据，来预测结果。其中包括了两大问题：\n  - **回归**（Regression）\n    给算法做一个数据集，包含正确答案，（比如房价-年），用线性/非线性回归方程拟合数据,预测数据。\n    ![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124639.png)\n  - **分类问题**（**逻辑回归**问题）（Classification/Logical regression）\n    用实数对出现的可能状况分类\n    （比如：1和0表示患乳腺癌/不患乳腺癌 ；1表示患乳腺癌A，2表示患乳腺癌B，0表示不患乳腺癌），在多维坐标系中（每一个维度表示不同的属性），然后用线性或非线性的函数将不同类的数据分开。\n    ![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124726.png)\n- **无监督学习**（Unsupervised）计算机自己学习，经典的算法分为两大类：\n  - **聚类算法**\n    对并不明确分类的数据集，计算机根据数据特征自动将数据分为几个簇\n    ![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124803.png)\n  - **鸡尾酒会算法**（Cocktail party）\n    略，这里只对鸡尾酒会问题和解决方法作一个概述：\n    鸡尾酒会问题是在计算机语音识别 领域的一个问题。\n    当前语音识别技术已经可以以较高精度识别一个人所讲的话，但是当说话的人数为两人或者多人时，语音识别率就会极大的降低，这一难题被称为鸡尾酒会问题。\n    对于的给定混合信号，分离出鸡尾酒会中 同时说话的每个人的独立信号。\n    鸡尾酒问题的解决方法是把两个收音器分别放在两个人的附近，每个收音器且与两个人的距离是不等距的，如此来分离两个人的声音。\n","source":"_posts/1. 线性回归/1.1. 什么是机器学习.md","raw":"---\ntitle: 1.1. 什么是机器学习\nmath: true\n\n---\n\n# 什么是机器学习\n\n## 机器学习的定义\n\n> A computer  program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.    ——Tom Mitchelle\n\n简言之，机器学习通过完成任务（T）得到经验（E），进而提升性能（P）。\n例如：一个自我对弈的跳棋学习机器：E ：自我对弈的棋局 T：下跳棋 P：与新对手玩跳棋时的获胜概率\n\n## 机器学习的主要算法类型\n\n- **监督学习**（Supervised）人教会计算机完成任务。根据统计数据做直线或曲线拟合/分离数据，来预测结果。其中包括了两大问题：\n  - **回归**（Regression）\n    给算法做一个数据集，包含正确答案，（比如房价-年），用线性/非线性回归方程拟合数据,预测数据。\n    ![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124639.png)\n  - **分类问题**（**逻辑回归**问题）（Classification/Logical regression）\n    用实数对出现的可能状况分类\n    （比如：1和0表示患乳腺癌/不患乳腺癌 ；1表示患乳腺癌A，2表示患乳腺癌B，0表示不患乳腺癌），在多维坐标系中（每一个维度表示不同的属性），然后用线性或非线性的函数将不同类的数据分开。\n    ![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124726.png)\n- **无监督学习**（Unsupervised）计算机自己学习，经典的算法分为两大类：\n  - **聚类算法**\n    对并不明确分类的数据集，计算机根据数据特征自动将数据分为几个簇\n    ![](https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124803.png)\n  - **鸡尾酒会算法**（Cocktail party）\n    略，这里只对鸡尾酒会问题和解决方法作一个概述：\n    鸡尾酒会问题是在计算机语音识别 领域的一个问题。\n    当前语音识别技术已经可以以较高精度识别一个人所讲的话，但是当说话的人数为两人或者多人时，语音识别率就会极大的降低，这一难题被称为鸡尾酒会问题。\n    对于的给定混合信号，分离出鸡尾酒会中 同时说话的每个人的独立信号。\n    鸡尾酒问题的解决方法是把两个收音器分别放在两个人的附近，每个收音器且与两个人的距离是不等距的，如此来分离两个人的声音。\n","slug":"1. 线性回归/1.1. 什么是机器学习","published":1,"date":"2021-02-06T03:54:46.295Z","updated":"2021-02-06T04:20:46.786Z","_id":"ckkt7mbt30000d40haoxy1w85","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"什么是机器学习\"><a href=\"#什么是机器学习\" class=\"headerlink\" title=\"什么是机器学习\"></a>什么是机器学习</h1><h2 id=\"机器学习的定义\"><a href=\"#机器学习的定义\" class=\"headerlink\" title=\"机器学习的定义\"></a>机器学习的定义</h2><blockquote>\n<p>A computer  program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.    ——Tom Mitchelle</p>\n</blockquote>\n<p>简言之，机器学习通过完成任务（T）得到经验（E），进而提升性能（P）。<br>例如：一个自我对弈的跳棋学习机器：E ：自我对弈的棋局 T：下跳棋 P：与新对手玩跳棋时的获胜概率</p>\n<h2 id=\"机器学习的主要算法类型\"><a href=\"#机器学习的主要算法类型\" class=\"headerlink\" title=\"机器学习的主要算法类型\"></a>机器学习的主要算法类型</h2><ul>\n<li><strong>监督学习</strong>（Supervised）人教会计算机完成任务。根据统计数据做直线或曲线拟合/分离数据，来预测结果。其中包括了两大问题：<ul>\n<li><strong>回归</strong>（Regression）<br>给算法做一个数据集，包含正确答案，（比如房价-年），用线性/非线性回归方程拟合数据,预测数据。<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124639.png\" alt=\"\"></li>\n<li><strong>分类问题</strong>（<strong>逻辑回归</strong>问题）（Classification/Logical regression）<br>用实数对出现的可能状况分类<br>（比如：1和0表示患乳腺癌/不患乳腺癌 ；1表示患乳腺癌A，2表示患乳腺癌B，0表示不患乳腺癌），在多维坐标系中（每一个维度表示不同的属性），然后用线性或非线性的函数将不同类的数据分开。<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124726.png\" alt=\"\"></li>\n</ul>\n</li>\n<li><strong>无监督学习</strong>（Unsupervised）计算机自己学习，经典的算法分为两大类：<ul>\n<li><strong>聚类算法</strong><br>对并不明确分类的数据集，计算机根据数据特征自动将数据分为几个簇<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124803.png\" alt=\"\"></li>\n<li><strong>鸡尾酒会算法</strong>（Cocktail party）<br>略，这里只对鸡尾酒会问题和解决方法作一个概述：<br>鸡尾酒会问题是在计算机语音识别 领域的一个问题。<br>当前语音识别技术已经可以以较高精度识别一个人所讲的话，但是当说话的人数为两人或者多人时，语音识别率就会极大的降低，这一难题被称为鸡尾酒会问题。<br>对于的给定混合信号，分离出鸡尾酒会中 同时说话的每个人的独立信号。<br>鸡尾酒问题的解决方法是把两个收音器分别放在两个人的附近，每个收音器且与两个人的距离是不等距的，如此来分离两个人的声音。</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"什么是机器学习\"><a href=\"#什么是机器学习\" class=\"headerlink\" title=\"什么是机器学习\"></a>什么是机器学习</h1><h2 id=\"机器学习的定义\"><a href=\"#机器学习的定义\" class=\"headerlink\" title=\"机器学习的定义\"></a>机器学习的定义</h2><blockquote>\n<p>A computer  program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.    ——Tom Mitchelle</p>\n</blockquote>\n<p>简言之，机器学习通过完成任务（T）得到经验（E），进而提升性能（P）。<br>例如：一个自我对弈的跳棋学习机器：E ：自我对弈的棋局 T：下跳棋 P：与新对手玩跳棋时的获胜概率</p>\n<h2 id=\"机器学习的主要算法类型\"><a href=\"#机器学习的主要算法类型\" class=\"headerlink\" title=\"机器学习的主要算法类型\"></a>机器学习的主要算法类型</h2><ul>\n<li><strong>监督学习</strong>（Supervised）人教会计算机完成任务。根据统计数据做直线或曲线拟合/分离数据，来预测结果。其中包括了两大问题：<ul>\n<li><strong>回归</strong>（Regression）<br>给算法做一个数据集，包含正确答案，（比如房价-年），用线性/非线性回归方程拟合数据,预测数据。<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124639.png\" alt=\"\"></li>\n<li><strong>分类问题</strong>（<strong>逻辑回归</strong>问题）（Classification/Logical regression）<br>用实数对出现的可能状况分类<br>（比如：1和0表示患乳腺癌/不患乳腺癌 ；1表示患乳腺癌A，2表示患乳腺癌B，0表示不患乳腺癌），在多维坐标系中（每一个维度表示不同的属性），然后用线性或非线性的函数将不同类的数据分开。<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124726.png\" alt=\"\"></li>\n</ul>\n</li>\n<li><strong>无监督学习</strong>（Unsupervised）计算机自己学习，经典的算法分为两大类：<ul>\n<li><strong>聚类算法</strong><br>对并不明确分类的数据集，计算机根据数据特征自动将数据分为几个簇<br><img src=\"https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124803.png\" alt=\"\"></li>\n<li><strong>鸡尾酒会算法</strong>（Cocktail party）<br>略，这里只对鸡尾酒会问题和解决方法作一个概述：<br>鸡尾酒会问题是在计算机语音识别 领域的一个问题。<br>当前语音识别技术已经可以以较高精度识别一个人所讲的话，但是当说话的人数为两人或者多人时，语音识别率就会极大的降低，这一难题被称为鸡尾酒会问题。<br>对于的给定混合信号，分离出鸡尾酒会中 同时说话的每个人的独立信号。<br>鸡尾酒问题的解决方法是把两个收音器分别放在两个人的附近，每个收音器且与两个人的距离是不等距的，如此来分离两个人的声音。</li>\n</ul>\n</li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}